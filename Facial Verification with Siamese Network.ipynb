{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e0784b-8e61-4980-b019-1f8e3b58a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613de65e-7022-4b8a-978f-9854226bcf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create -n tf_env python=3.8 -y\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a9df7f-d108-4924-8b8d-f99760cdee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff246aa8-3d83-4c4d-a36f-a55e02f607ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7efc7ba-08d4-40c9-b87e-a51161a5065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install tensorflow opencv-python matplotlib numpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01647d4e-0a8a-4c51-83c4-7ffa92386519",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import tensorflow as tf\n",
    "\n",
    "# Print TensorFlow version\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "\n",
    "# Check for GPU availability\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPUs Available:\", gpus)\n",
    "else:\n",
    "    print(\"No GPUs Available.\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ec6bb5-13bf-4daa-a917-cd3079619923",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6acc4f3-9089-41ca-8816-652e4d426e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow dependencies - Functional API\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592abc7-9765-4460-a441-353166667cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "'''gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7f6919-2eb6-40db-ac72-d872c5406ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "POS_PATH = os.path.join('data', 'positive')\n",
    "NEG_PATH = os.path.join('data', 'negative')\n",
    "ANC_PATH = os.path.join('data', 'anchor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57883194-3ac2-4717-9f50-3bed654efdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the directories\n",
    "os.makedirs(POS_PATH,exist_ok=True)\n",
    "os.makedirs(NEG_PATH,exist_ok=True)\n",
    "os.makedirs(ANC_PATH,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e99e5f-3a98-4e0d-92c3-3b7dcbbcb5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://vis-www.cs.umass.edu/lfw/lfw.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b241213c-39ab-402b-a1f5-f6b6cfd45561",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Uncompress Tar GZ Labelled Faces in the Wild Dataset\n",
    "!tar -xf lfw.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d8431a-d251-4d7b-839e-e8b5b69b0f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move LFW Images to the following repository data/negative\n",
    "for directory in os.listdir('lfw'):\n",
    "    for file in os.listdir(os.path.join('lfw', directory)):\n",
    "        EX_PATH = os.path.join('lfw', directory, file)\n",
    "        NEW_PATH = os.path.join(NEG_PATH, file)\n",
    "        \n",
    "        os.replace(EX_PATH, NEW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb191ed5-d1d7-41f9-bd0c-e52bc52d81e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import uuid library to generate unique image names\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5d7ca5-fc3a-409d-a3cf-7dd5d57b0cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a61f52b-34e4-4793-b648-d5f0bcae1232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection to the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened(): \n",
    "    ret, frame = cap.read()\n",
    "\n",
    "   \n",
    "    # Cut down frame to 250x250px\n",
    "    frame = frame[120:120+250,200:200+250, :]\n",
    "    \n",
    "    # Collect anchors \n",
    "    if cv2.waitKey(1) & 0XFF == ord('a'):\n",
    "        # Create the unique file path \n",
    "        imgname = os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "        # Write out anchor image\n",
    "        cv2.imwrite(imgname, frame)\n",
    "    \n",
    "    # Collect positives\n",
    "    if cv2.waitKey(1) & 0XFF == ord('p'):\n",
    "        # Create the unique file path \n",
    "        imgname = os.path.join(POS_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "        # Write out positive image\n",
    "        cv2.imwrite(imgname, frame)\n",
    "    \n",
    "    # Show image back to screen\n",
    "    cv2.imshow('Image Collection', frame)\n",
    "    \n",
    "    # Breaking gracefully\n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "# Release the webcam\n",
    "cap.release()\n",
    "# Close the image show frame\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e7a22-01bd-454f-a9bb-543b82928e15",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c32c7-a4a8-474d-ab69-0f5906497702",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set the paths for the image directories\n",
    "ANC_PATH = \"data/anchor\"  # Directory containing anchor images\n",
    "POS_PATH = \"data/positive\"  # Directory containing positive images\n",
    "\n",
    "# Data augmentation function\n",
    "def data_aug(img):\n",
    "    data = []\n",
    "    for i in range(9):  # Generate 9 augmented images\n",
    "        img_aug = tf.image.stateless_random_brightness(img, max_delta=0.02, seed=(1, 2))\n",
    "        img_aug = tf.image.stateless_random_contrast(img_aug, lower=0.6, upper=1, seed=(1, 3))\n",
    "        img_aug = tf.image.stateless_random_flip_left_right(img_aug, seed=(np.random.randint(100), np.random.randint(100)))\n",
    "        img_aug = tf.image.stateless_random_jpeg_quality(img_aug, min_jpeg_quality=90, max_jpeg_quality=100, seed=(np.random.randint(100), np.random.randint(100)))\n",
    "        img_aug = tf.image.stateless_random_saturation(img_aug, lower=0.9, upper=1, seed=(np.random.randint(100), np.random.randint(100)))\n",
    "        \n",
    "        data.append(img_aug)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Function to augment images in a given directory\n",
    "def augment_images_in_directory(directory):\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.jpg'):  # Process only jpg files\n",
    "            img_path = os.path.join(directory, file_name)\n",
    "            img = cv2.imread(img_path)\n",
    "             if img is None:\n",
    "                raise ValueError(f\"Error: Could not read the image at {img_path}\")\n",
    "\n",
    "            # Apply data augmentation\n",
    "            augmented_images = data_aug(img)\n",
    "\n",
    "            # Save the augmented images\n",
    "            for image in augmented_images:\n",
    "                cv2.imwrite(os.path.join(directory, '{}.jpg'.format(uuid.uuid1())), image.numpy())\n",
    "\n",
    "            #print(f\"Augmented images saved for {file_name} in {directory}\")\n",
    "\n",
    "# Augment images in both directories\n",
    "augment_images_in_directory(ANC_PATH)\n",
    "augment_images_in_directory(POS_PATH)\n",
    "\n",
    "print(\"Data augmentation completed for all images in both directories.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d7aaf2-e7cf-46bf-9fed-f7e0996c2410",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''def data_aug(img):\n",
    "    data = []\n",
    "    for i in range(9):\n",
    "        img = tf.image.stateless_random_brightness(img, max_delta=0.02, seed=(1,2))\n",
    "        img = tf.image.stateless_random_contrast(img, lower=0.6, upper=1, seed=(1,3))\n",
    "        # img = tf.image.stateless_random_crop(img, size=(20,20,3), seed=(1,2))\n",
    "        img = tf.image.stateless_random_flip_left_right(img, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "        img = tf.image.stateless_random_jpeg_quality(img, min_jpeg_quality=90, max_jpeg_quality=100, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "        img = tf.image.stateless_random_saturation(img, lower=0.9,upper=1, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "            \n",
    "        data.append(img)\n",
    "    \n",
    "    return data'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3a8307-eecc-43a8-bead-38508b40ff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79f74db-04cf-4e0e-a41a-db17496e1616",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''img_path = os.path.join(ANC_PATH, 'fdb9723d-89e8-11ef-ad57-e8fb1c369fc0.jpg')\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "augmented_images = data_aug(img)\n",
    "\n",
    "for image in augmented_images:\n",
    "    cv2.imwrite(os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1())), image.numpy())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e4607f-e841-4ae9-9e20-c3d2a0cd6a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for file_name in os.listdir(os.path.join(POS_PATH)):\n",
    "    img_path = os.path.join(POS_PATH, file_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    augmented_images = data_aug(img) \n",
    "    \n",
    "    for image in augmented_images:\n",
    "        cv2.imwrite(os.path.join(POS_PATH, '{}.jpg'.format(uuid.uuid1())), image.numpy())'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf93c4ef-b478-4c6c-8678-d96adc60a7bf",
   "metadata": {},
   "source": [
    "##  3. Load and Preprocess Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab777d8-37ae-4fae-9261-7260c0f08b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15081b6e-e9eb-4fe2-ba9e-feff580fef79",
   "metadata": {},
   "source": [
    "###  3.1 Get Image Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe9b49b-bd38-4855-897b-8ec4a5486ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor=tf.data.Dataset.list_files(ANC_PATH+'/*.jpg').take(300)\n",
    "positive=tf.data.Dataset.list_files(POS_PATH+'/*.jpg').take(300)\n",
    "negative=tf.data.Dataset.list_files(NEG_PATH+'/*.jpg').take(300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1320a52-2e6d-421a-aee6-7e084462e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_test=anchor.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907c7d90-6a06-42eb-be53-e45431418851",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir_test.next())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf208b-096b-4aec-b300-100349629066",
   "metadata": {},
   "source": [
    "3.2 Preprocessing-scale and resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400369ee-0901-4abc-8d11-b22d15a13aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "\n",
    "    #Read in image in file path \n",
    "    byte_img=tf.io.read_file(file_path)\n",
    "\n",
    "    #load in the image\n",
    "    img=tf.io.decode_jpeg(byte_img)\n",
    "\n",
    "    #resizing the image to be 100x100x3\n",
    "    img=tf.image.resize(img,(100,100))\n",
    "    img=img/255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdd3824-26b4-4a76-b6eb-3cd168f74be0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img=preprocess('data\\\\anchor\\\\dedb3ff2-8894-11ef-8a22-e8fb1c369fc0.jpg')\n",
    "plt.imshow(img)\n",
    "img.numpy().min()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9eabaa9a-a391-4f73-a558-f63272c96730",
   "metadata": {},
   "source": [
    "#dataset.map(preprocess)\n",
    "anchor_processed = anchor.map(preprocess)\n",
    "positive_processed = positive.map(preprocess)\n",
    "negative_processed = negative.map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540fbfa8-3923-4bff-a046-763842b882e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Take one element from the dataset and check its shape\n",
    "for img in anchor_processed.take(1):\n",
    "    print(img.shape)'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b16ce1a-4922-4ffa-8008-5a7c99af4a05",
   "metadata": {},
   "source": [
    "#### 3.3 Create labelled dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b86a4f-4066-433a-8c50-a6d545fd26b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "positives=tf.data.Dataset.zip(anchor,positive,tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))) \n",
    "negatives=tf.data.Dataset.zip(anchor,negative,tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))) \n",
    "data=positives.concatenate(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b71e3d6-2489-490e-b8f5-dcab8f4f7538",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f0a4db-9759-4cf4-b815-06b661c5afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex=samples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc72636-f177-43b7-803d-5af5153ac1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b61fc1-9f24-404d-a903-54b379770910",
   "metadata": {},
   "source": [
    "3.4 Build Train and Test Partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37551cb-cadc-4595-b87b-71d770a48a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_twin(input_image,validation_image,label):\n",
    "    return (preprocess(input_image),preprocess(validation_image),label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c8dc1-dfbf-45ea-bc61-16bdd6d2995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=preprocess_twin(*ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabd18dc-f47d-4880-957d-fbcb40b2c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcba4b17-096b-4248-9e2f-dda60e6c2754",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b83d01f-6823-48e3-8c9d-cb4be955d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b79b61a-07a8-431d-9e75-57f6f207a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39978288-9d0c-4b6e-9b0d-51fba013118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build dataloader pipeline\n",
    "data=data.map(preprocess_twin)\n",
    "data=data.cache()\n",
    "data=data.shuffle(buffer_size=1024)\n",
    "#round(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27633ce9-c475-431f-80f1-a31948339bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing whether the suffling is occurred or not and check the output\n",
    "'''samples=data.as_numpy_iterator()\n",
    "samp=samples.next()\n",
    "len(samp)\n",
    "plt.imshow(samp[0])\n",
    "plt.imshow(samp[1])\n",
    "plt.imshow(samp[2])'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010cb4fe-ff55-461d-812f-e1277e511763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebdaf96-7cdd-41cd-96e7-2dcc55e873e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training partition\n",
    "train_data=data.take(round(len(data)*0.8))\n",
    "train_data=train_data.batch(16)\n",
    "train_data=train_data.prefetch(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ea7a9-b513-4996-a3f2-062323baa566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking whether the batch_size is 16 or not\n",
    "'''train_samples=train_data.as_numpy_iterator()\n",
    "t_samp=train_samples.next()\n",
    "len(t_samp[0])'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1504731-2aca-429c-bf09-efbec41a8fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing partition\n",
    "test_data=data.skip(round(len(data)*0.8))\n",
    "test_data=data.take(round(len(data)*0.2))\n",
    "test_data=test_data.batch(16)\n",
    "test_data=test_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc6327-13d2-46ad-af6d-63397f4cdcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking whether the batch_size is 16 or not\n",
    "'''test_samples=test_data.as_numpy_iterator()\n",
    "t_samp=test_samples.next()\n",
    "len(t_samp[0])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a98034-1d9d-466c-bffd-892d56bbe806",
   "metadata": {},
   "source": [
    "## 4. Model Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c10716f-a0b9-4311-8904-880ed7cc041b",
   "metadata": {},
   "source": [
    "### 4.1 Build Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b014c37c-7510-4ee4-9bbf-e6bad9851a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(100,100,3), name='input_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0093ee29-1c53-4d70-a78d-325feecc8ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = Conv2D(64, (10,10), activation='relu')(inp)\n",
    "m1 = MaxPooling2D(64, (2,2), padding='same')(c1)\n",
    "c2 = Conv2D(128, (7,7), activation='relu')(m1)\n",
    "m2 = MaxPooling2D(64, (2,2), padding='same')(c2)\n",
    "c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
    "m3 = MaxPooling2D(64, (2,2), padding='same')(c3)\n",
    "c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
    "f1 = Flatten()(c4)\n",
    "d1 = Dense(4096, activation='sigmoid')(f1)\n",
    "mod = Model(inputs=[inp], outputs=[d1], name='embedding')\n",
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f3353a-6be1-4711-a2a3-3a6eb437d80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding(): \n",
    "    inp = Input(shape=(100,100,3), name='input_image')\n",
    "    \n",
    "    # First block\n",
    "    c1 = Conv2D(64, (10,10), activation='relu')(inp)\n",
    "    m1 = MaxPooling2D(64, (2,2), padding='same')(c1)\n",
    "    \n",
    "    # Second block\n",
    "    c2 = Conv2D(128, (7,7), activation='relu')(m1)\n",
    "    m2 = MaxPooling2D(64, (2,2), padding='same')(c2)\n",
    "    \n",
    "    # Third block \n",
    "    c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
    "    m3 = MaxPooling2D(64, (2,2), padding='same')(c3)\n",
    "    \n",
    "    # Final embedding block\n",
    "    c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
    "    f1 = Flatten()(c4)\n",
    "    d1 = Dense(4096, activation='sigmoid')(f1)\n",
    "    \n",
    "    \n",
    "    return Model(inputs=[inp], outputs=[d1], name='embedding')\n",
    "\n",
    "\n",
    "embedding = make_embedding()\n",
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c0e307-4f6f-44af-b235-edc26552ffe2",
   "metadata": {},
   "source": [
    "### 4.2 Build Distance Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42ed49c-8b86-4baf-8b0e-348832cdafef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese L1 Distance class\n",
    "class L1Dist(Layer):\n",
    "    \n",
    "    # Init method - inheritance\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "       \n",
    "    # Magic happens here - similarity calculation\n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "         # Ensure the inputs are tensors\n",
    "        input_embedding = tf.convert_to_tensor(input_embedding)\n",
    "        validation_embedding = tf.convert_to_tensor(validation_embedding)\n",
    "        \n",
    "        return tf.math.abs(input_embedding - validation_embedding)\n",
    "        #return tf.sqrt(tf.reduce_sum(tf.square(input_embedding - validation_embedding), axis=1))\n",
    "l1 = L1Dist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855a7a70-1d62-472d-9469-238b1ad8b680",
   "metadata": {},
   "source": [
    "#### 4.3 Make Siamese Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b336a0-d348-4ee3-b668-dc156e8774c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''input_image = Input(name='input_img', shape=(100,100,3))\n",
    "validation_image = Input(name='validation_img', shape=(100,100,3))\n",
    "inp_embedding = embedding(input_image)\n",
    "val_embedding = embedding(validation_image)\n",
    "siamese_layer = l1(inp_embedding,val_embedding)\n",
    "#print(len(siamese_layer)) \n",
    "print(siamese_layer.shape)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a1934e-7d2a-4401-82c8-88fc0195411e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640623b5-98fc-4199-a8b9-851c3bd16495",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''distances=siamese_layer(inp_embedding, val_embedding)\n",
    "classifier = Dense(1, activation='sigmoid')(distances)\n",
    "classifier'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c47b6-cad2-4603-831d-890b77fd85ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''siamese_network = Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')\n",
    "siamese_network.summary()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058077fa-9f52-4a70-b883-85af01501315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model(): \n",
    "    \n",
    "    # Anchor image input in the network\n",
    "    input_image = Input(name='input_img', shape=(100,100,3))\n",
    "    \n",
    "    # Validation image in the network \n",
    "    validation_image = Input(name='validation_img', shape=(100,100,3))\n",
    "    \n",
    "    # Combine siamese distance components\n",
    "    siamese_layer = L1Dist()\n",
    "    siamese_layer._name = 'distance'\n",
    "    distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
    "    \n",
    "    # Classification layer \n",
    "    classifier = Dense(1, activation='sigmoid')(distances)\n",
    "    \n",
    "    return Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f9a0de-2f71-48fd-9e72-9aa6b9f675bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = make_siamese_model()\n",
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fcd397-8f9c-4fdf-9847-f19c775b624e",
   "metadata": {},
   "source": [
    "\r",
    "## 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efbbc1a-0498-45a0-90c1-8016bac57714",
   "metadata": {},
   "source": [
    "#### 5.1 Setup Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09dfcdf-be2a-44a4-905a-d0aded822e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_loss = tf.losses.BinaryCrossentropy()\n",
    "opt = tf.keras.optimizers.Adam(0.0001) # 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a2eb82-ae9a-480c-b396-38c554169405",
   "metadata": {},
   "source": [
    "#### 5.2 Establish Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f659a918-9897-4cae-85ff-733301c1618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b2c9d6-2eab-42bf-a3b9-f5fc3898cffe",
   "metadata": {},
   "source": [
    "#### 5.3 Build Train Step Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f2773f-306f-4572-82f3-a52a31783e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = train_data.as_numpy_iterator()\n",
    "batch_1 = test_batch.next()\n",
    "X = batch_1\n",
    "y=batch_1[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa1c63c-f7cc-4e13-bf84-cd06b0be87a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=batch_1[:2]\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7998b335-0d48-4b5c-873f-22519d372e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch):\n",
    "    \n",
    "    # Record all of our operations \n",
    "    with tf.GradientTape() as tape:     \n",
    "        # Get anchor and positive/negative image\n",
    "        X = batch[:2]\n",
    "        # Get label\n",
    "        y = batch[2]\n",
    "        \n",
    "        # Forward pass\n",
    "        \n",
    "        yhat = siamese_model(X, training=True)\n",
    "        \n",
    "        #yhat=tf.squeeze(yhat)\n",
    "        # Reshape yhat to match y\n",
    "        yhat = tf.reshape(yhat, y.shape)\n",
    "        # Ensure yhat has the same shape as y (reshape predictions)\n",
    "        #yhat = tf.reshape(yhat, y.shape)  # Reshape to match target shape\n",
    "        # Calculate loss\n",
    "        loss = binary_cross_loss(y, yhat)\n",
    "     print(loss)\n",
    "        \n",
    "    # Calculate gradients\n",
    "    grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
    "    \n",
    "    # Calculate updated weights and apply to siamese model\n",
    "    opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
    "    \n",
    "    # Return loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646a22f5-004d-4c4e-a9ed-557ea5c6df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''@tf.function\n",
    "def train_step(batch):\n",
    "    \n",
    "    # Record all of our operations \n",
    "    with tf.GradientTape() as tape:     \n",
    "        # Get anchor and positive/negative image\n",
    "        X = batch[:2]\n",
    "        # Get label\n",
    "        y = batch[2]\n",
    "        \n",
    "        # Forward pass\n",
    "        yhat = siamese_model(X, training=True)\n",
    "        # Reshape y to ensure it matches the shape of yhat\n",
    "        y = tf.reshape(y, (-1, 1))  # Ensure y has shape [batch_size, 1]\n",
    "        # Calculate loss\n",
    "        loss = binary_cross_loss(y, yhat)\n",
    "    #print(loss)\n",
    "        \n",
    "    # Calculate gradients\n",
    "    grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
    "    \n",
    "    # Calculate updated weights and apply to siamese model\n",
    "    opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
    "        \n",
    "    # Return loss\n",
    "    return loss'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f31937b-af6d-4ca0-9a5a-40aba748e039",
   "metadata": {},
   "source": [
    "#### 5.4 Build Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87826fbb-9483-4d92-969e-37cd421bb680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, EPOCHS):\n",
    "    # Loop through epochs\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        print('\\n Epoch {}/{}'.format(epoch, EPOCHS))\n",
    "        progbar = tf.keras.utils.Progbar(len(data))\n",
    "        \n",
    "        # Loop through each batch\n",
    "        for idx, batch in enumerate(data):\n",
    "            # Run train step here\n",
    "            train_step(batch)\n",
    "            progbar.update(idx+1)\n",
    "        \n",
    "        # Save checkpoints\n",
    "        if epoch % 10 == 0: \n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d7a452-dcec-4297-982e-38693360e583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fc10485-1f12-4940-ab73-641209e31157",
   "metadata": {},
   "source": [
    "5.5 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559086a-e024-4106-b119-8ab5b65fcfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 60\n",
    "train(train_data, EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba847a-0db4-4b17-b36f-425dddcc6f9a",
   "metadata": {},
   "source": [
    "## 6.Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7936f681-f717-448b-a7b1-9da36569292a",
   "metadata": {},
   "source": [
    "#### 6.1 import metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f26375-dcb6-441f-bd35-b58dc40b68fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import metric calculations\n",
    "from tensorflow.keras.metrics import Precision,Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9650b9bc-ad5f-4f43-b84e-c55ca536fd45",
   "metadata": {},
   "source": [
    "#### 6.2make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e77116-3674-4b81-b4cd-e2df3784306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get A batch of test data\n",
    "test_input,test_val,y_true=test_data.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f003763e-e7a0-4ffa-8751-0c6640bad37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions\n",
    "y_hat=siamese_model.predict([test_input,test_val])\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b50a390-2142-4c98-9805-9245ead6be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#post processing the results\n",
    "y_hat_flat=y_hat.flatten()\n",
    "bi_predictions=[1 if prediction > 0.8 else 0 for prediction in y_hat_flat]\n",
    "bi_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b337fa02-4f1d-4bbb-a94f-fd9881d978c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5212caa-27e1-4dcc-ae94-01b4127905d0",
   "metadata": {},
   "source": [
    "#### 6.3 Calculate Metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c0347-1e11-4ffd-9064-68f616e112f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Recall()\n",
    "m.update_state(y_true,y_hat_flat)\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cdd392-2a4b-4c53-a42e-8d046515365a",
   "metadata": {},
   "source": [
    "#### 6.4 Viz results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b996fb06-2de1-4e60-a0e0-dfc9834ce7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set plot size\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "#set 1st subplot\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_input[2])\n",
    "\n",
    "#set 2nd subplot\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(test_val[2])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89f5fa3-394c-43ce-a31f-04871c98fa2e",
   "metadata": {},
   "source": [
    "## 7.Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd733b-0299-48ac-9a27-a3d9c3ea5245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save weights\n",
    "siamese_model.save(\"siamesemodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ed909-e566-430e-a3d3-905103403940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload the model\n",
    "model= tf.keras.models.load_model(\"siamesemodel.h5\",\n",
    "                                custom_objects={\"L1Dist\":L1Dist})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690b99f1-e817-442c-9aad-69225049a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with reloaded model\n",
    "siamese_model.predict([test_input, test_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25d744d-a24d-48b0-83b1-3e6913bef9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View model summary\n",
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a28263-b0f7-4e50-8cd8-6926e3b96ac4",
   "metadata": {},
   "source": [
    "## 8. Real Time Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba462bff-22d6-4b66-a11b-ebf77d5aec4d",
   "metadata": {},
   "source": [
    "#### 8.1 Verification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285c8817-a768-4712-b1cd-8b50fcdeea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(os.path.join('application_data', 'verification_images'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44916ef-bbc1-4c63-90ba-93057bcafb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join('application_data', 'input_image', 'input_image.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e8aebd-fd7b-4e72-8d18-b1c249c9a423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(model, detection_threshold, verification_threshold):\n",
    "    # Build results array\n",
    "    results = []\n",
    "    for image in os.listdir(os.path.join('application_data', 'verification_images')):\n",
    "        input_img = preprocess(os.path.join('application_data', 'input_image', 'input_image.jpg'))\n",
    "        validation_img = preprocess(os.path.join('application_data', 'verification_images', image))\n",
    "        \n",
    "        # Make Predictions \n",
    "        result = model.predict(list(np.expand_dims([input_img, validation_img], axis=1)))\n",
    "        results.append(result)\n",
    "    \n",
    "    # Detection Threshold: Metric above which a prediciton is considered positive \n",
    "    detection = np.sum(np.array(results) > detection_threshold)\n",
    "    \n",
    "    # Verification Threshold: Proportion of positive predictions / total positive samples \n",
    "    verification = detection / len(os.listdir(os.path.join('application_data', 'verification_images'))) \n",
    "    #verified = verification > verification_threshold\n",
    "    \n",
    "    return results, verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e7de97-ed69-4b58-8ae0-34202ddd5b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe20044e-3d60-4b5b-9536-da9e6bae8f5c",
   "metadata": {},
   "source": [
    "#### 8.2 OpenCV Real Time Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83847c50-1a7d-490a-b2d7-bf3230f21147",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = frame[120:120+250,200:200+250, :]\n",
    "    \n",
    "    cv2.imshow('Verification', frame)\n",
    "    \n",
    "    # Verification trigger\n",
    "    if cv2.waitKey(10) & 0xFF == ord('v'):\n",
    "        # Save input image to application_data/input_image folder \n",
    "#         hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "#         h, s, v = cv2.split(hsv)\n",
    "\n",
    "#         lim = 255 - 10\n",
    "#         v[v > lim] = 255\n",
    "#         v[v <= lim] -= 10\n",
    "        \n",
    "#         final_hsv = cv2.merge((h, s, v))\n",
    "#         img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        cv2.imwrite(os.path.join('application_data', 'input_image', 'input_image.jpg'), frame)\n",
    "        # Run verification\n",
    "        results, verification = verify(siamese_model, 0.99, 0.7)\n",
    "        #print(verified)\n",
    "        print(verification)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "verification.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9059ad46-07fa-49e6-a6d8-41a7a262887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81d45c6-64a6-4685-8ff8-4984b173ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.squeeze(results) > 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25070e38-8b6e-49f5-afbb-878f35cc357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8071131-64ac-4383-8d56-a67bcaf2d1eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
